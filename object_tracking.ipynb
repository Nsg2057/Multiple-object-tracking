{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Understand the problem and setup environment**\n",
        "\n",
        "We're trying to track the trajectory of the ball object across the video. Here we need to detect the ball object first \n",
        "\n",
        "\n",
        "Kalman Filter - "
      ],
      "metadata": {
        "id": "tYeiKlMbPyOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxm6X8NTTBqw"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorflow==2.2.0\n",
        "!pip3 install keras==2.2.4\n",
        "!pip3 install imageai --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AWZvFv2eLb5C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow.keras\n",
        "from imageai.Detection import ObjectDetection\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f01QjS1DoAIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2mHGng50KKmr"
      },
      "outputs": [],
      "source": [
        "class KalmanFilter(object):\n",
        "    def __init__(self, dt, a_x,a_y, std_acc, x_std_meas, y_std_meas):\n",
        "        # Define sampling time\n",
        "        self.dt = dt\n",
        "        # Define the  control input variables\n",
        "        self.u = np.matrix([[a_x],[a_y]])\n",
        "        # Intial State\n",
        "        self.x = np.matrix([[0], [0], [0], [0]])\n",
        "        # Define the State Transition Matrix A\n",
        "        self.A = np.matrix([[1, 0, self.dt, 0],\n",
        "                            [0, 1, 0, self.dt],\n",
        "                            [0, 0, 1, 0],\n",
        "                            [0, 0, 0, 1]])\n",
        "        # Define the Control Input Matrix B\n",
        "        self.B = np.matrix([[(self.dt**2)/2, 0],\n",
        "                            [0, (self.dt**2)/2],\n",
        "                            [self.dt,0],\n",
        "                            [0,self.dt]])\n",
        "        # Define Measurement Mapping Matrix\n",
        "        self.H = np.matrix([[1, 0, 0, 0],\n",
        "                            [0, 1, 0, 0]])\n",
        "        #Initial Process Noise Covariance\n",
        "        self.Q = np.matrix([[(self.dt**4)/4, 0, (self.dt**3)/2, 0],\n",
        "                            [0, (self.dt**4)/4, 0, (self.dt**3)/2],\n",
        "                            [(self.dt**3)/2, 0, self.dt**2, 0],\n",
        "                            [0, (self.dt**3)/2, 0, self.dt**2]]) * std_acc**2\n",
        "        #Initial Measurement Noise Covariance\n",
        "        self.R = np.matrix([[x_std_meas**2,0],\n",
        "                           [0, y_std_meas**2]])\n",
        "        #Initial Covariance Matrix\n",
        "        self.P = np.eye(self.A.shape[1])\n",
        "    def predict_next_position(self):\n",
        "        #State Transition Matrix and Control input matrix is used to predict the next state of the object\n",
        "        self.x = np.dot(self.A, self.x) + np.dot(self.B, self.u)\n",
        "        self.P = np.dot(np.dot(self.A, self.P), self.A.T) + self.Q\n",
        "        return self.x[0:2]\n",
        "    def update_object_position(self, z):\n",
        "        #Update the state of the object using Measurement Mapping Matrix and Predicted position from the predict function\n",
        "        S = np.dot(self.H, np.dot(self.P, self.H.T)) + self.R\n",
        "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))  #Eq.(11)\n",
        "        self.x = np.round(self.x + np.dot(K, (z - np.dot(self.H, self.x))))   #Eq.(12)\n",
        "        I = np.eye(self.H.shape[1])\n",
        "        # Update error covariance matrix\n",
        "        self.P = (I - (K * self.H)) * self.P   #Eq.(13)\n",
        "        return self.x[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vq2oVHfeaU-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 - In this part we're detecting the ball object from each frame of the video. Here we are using YOLOV3 as object detection model. This model has been trained to detect objects using COCO dataset. This detects the ball object with class = \"sports ball\"."
      ],
      "metadata": {
        "id": "EE7v5FLzxqXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This method detects the ball object in the image provided, calculates the centroid of the object and returns the centroid of the detected object\n",
        "def object_detection(image):\n",
        "  current_directory = os.getcwd()\n",
        "  detector = ObjectDetection() #Initializing the detector object  \n",
        "  detector.setModelTypeAsYOLOv3() #Set the model type for the detector as YOLOV3\n",
        "  detector.setModelPath('drive/MyDrive/attachments/yolo.h5') #set the model path\n",
        "  detector.loadModel() #loading the model\n",
        "  \n",
        "  #detect the objects from the image\n",
        "  detections = detector.detectObjectsFromImage(input_image = image, input_type = \"array\", output_image_path=os.path.join(current_directory , \"output.jpg\"))\n",
        "  centers = [] #Array to store the centroid of the detected\n",
        "  for eachObject in detections:\n",
        "      x1,y1,x2,y2 = eachObject[\"box_points\"] #get the bounding box coordinates of the detected object\n",
        "      cv2.rectangle(image, (x1,y1), (x2,y2), (0,0,255), 2) #draw rectangle around the detected object\n",
        "      cv2.putText(image,\"Bounding Box\",(x1+15,y1+15), 0,0.5,(0,255,0), 2)\n",
        "      cx = int((x1+x2)/2)\n",
        "      cy = int((y1+y2)/2)\n",
        "      inputCentroids = (cx,cy) #Calculating centroid of the bounding box\n",
        "      centers.append(np.array([[cx],[cy]]))\n",
        "  return centers #return the centroid of the detected object\n"
      ],
      "metadata": {
        "id": "LKmbeEK4Yy6r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SINGLE OBJECT TRACKING-**\n",
        "In this part, we're initializing the KF Object with initial values for time sampling time - time taken for 1 cycle. \n",
        "we're reading the video frame by frame using CV2 video capture. while the cv2.videoCapture method returns true, pass the frame through detect_object() method and get the centroid of the detected object.\n",
        "if the centroid is detected then pass those to update function of the kalman filter. the use the predice function to predict the next state of the detected centroid. This prediction works even when the object goes behind the paper. \n",
        "Finally we are using Cv2 video writer to add all the frames to a video. "
      ],
      "metadata": {
        "id": "7ipj55oNdzb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "HiSpeed= 100\n",
        "ControlSpeedVar = 30  \n",
        "    #Create KalmanFilter object KF\n",
        "    #KalmanFilter(dt, u_x, u_y, std_acc, x_std_meas, y_std_meas)\n",
        "KF = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)\n",
        "    # array to store the images to output to a video\n",
        "img_array = []\n",
        "\n",
        "VideoCap = cv2.VideoCapture('drive/MyDrive/attachments/ball.mp4')\n",
        "while(True):\n",
        " \n",
        "  ret, image = VideoCap.read()\n",
        "        # Detect object\n",
        "  if ret is False:\n",
        "      break\n",
        "  centers = object_detection(image)\n",
        "  if (len(centers) > 0):\n",
        "        # If centroids are detected then track the ball object\n",
        "            # Draw the detected circle\n",
        "    cv2.circle(image, (int(centers[0][0]), int(centers[0][1])), 10, (0, 191, 255), 2)\n",
        "    cv2.putText(image, \"Centroid\", (centers[0][0] + 15, centers[0][1] - 15), 0, 0.5, (0,191,255), 2)\n",
        "    # Update\n",
        "    (x1, y1) = KF.update_object_position(centers[0])\n",
        "    cv2.rectangle(image, (x1 - 15, y1 - 15), (x1 + 15, y1 + 15), (255, 0, 0), 2)\n",
        "    cv2.putText(image, \"Estimated position Position\", (x1 + 15, y1), 0, 0.5, (255, 0, 0), 2)\n",
        "            # Predict\n",
        "  (x, y) = KF.predict_next_position()\n",
        "            # Draw a rectangle as the predicted object position\n",
        "  cv2.rectangle(image, (x - 15, y - 15), (x + 15, y + 15), (255, 0, 0), 2)\n",
        "  cv2.putText(image, \"Predicted Position\", (x + 15, y), 0, 0.5, (255, 0, 0), 2)  \n",
        "  \n",
        "  height, width, layers = image.shape\n",
        "  size = (width,height)\n",
        "  img_array.append(image)\n",
        "\n",
        "  if cv2.waitKey(2) & 0xFF == ord('q'):\n",
        "            VideoCap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "            break\n",
        "  cv2.waitKey(HiSpeed-ControlSpeedVar+1)\n",
        "\n",
        "out = cv2.VideoWriter('single_object_tracking_output.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()"
      ],
      "metadata": {
        "id": "DWpydPDwfN3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTIPLE OBJECT TRACING"
      ],
      "metadata": {
        "id": "dFjGYWu3sF4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "KF1 = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)\n",
        "color = (255, 0, 0)\n",
        "thickness = 2\n",
        "current_directory = os.getcwd()\n",
        "detector = ObjectDetection()\n",
        "detector.setModelTypeAsYOLOv3()\n",
        "detector.setModelPath('drive/MyDrive/attachments/yolo.h5')\n",
        "detector.loadModel()\n",
        "img_array = []\n",
        "vidcap = cv2.VideoCapture('drive/MyDrive/attachments/multiple objects.avi')\n",
        "success,frame = vidcap.read()\n",
        "while success:\n",
        "    ret,image = vidcap.read()\n",
        "    \n",
        "    if ret is False:\n",
        "      break\n",
        "    height, width, layers = image.shape\n",
        "    size = (width,height)\n",
        "    detections = detector.detectObjectsFromImage(input_image = image, input_type = \"array\", output_image_path=os.path.join(current_directory , \"output.jpg\"))\n",
        "   \n",
        "    for eachObject in detections:\n",
        "      inputCentroids = np.zeros((len(detections), 2), dtype=\"int\")\n",
        "      x1,y1,x2,y2 = eachObject[\"box_points\"]\n",
        "      \n",
        "     #Calculate the centroid of the object to \n",
        "      cx = int((x1+x2)/2)\n",
        "      cy = int((y1+y2)/2)\n",
        "      inputCentroids = (cx,cy)   \n",
        "\n",
        "      cv2.rectangle(image,(x1,y1),(x2,y2), color, thickness)\n",
        "      cv2.putText(image, \"Bounding box\", (x1+15,y1+15), 0, 0.5, (0,255,0), 2)\n",
        "      cv2.circle(image, (cx,cy), 10, (0,0,255), thickness)\n",
        "      cv2.putText(image, \"Centroid\", (cx + 15, cy - 15), 0, 0.5, (0,255,0), 2)\n",
        "      \n",
        "       \n",
        "      img_array.append(image)\n",
        "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
        "     vidcap.release()\n",
        "     cv2.destroyAllWindows()\n",
        "     break\n",
        "out = cv2.VideoWriter('Multiple_Obj_detection_output.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZKL7eq-rWjX",
        "outputId": "b879f1b3-a902-488b-c9f2-8729980934b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n",
            "Inside Loop\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Single_object_tracking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}